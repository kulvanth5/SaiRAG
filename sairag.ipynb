{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7733364,"sourceType":"datasetVersion","datasetId":4519065},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install qdrant-client sentence_transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/sai-literature/modified.csv')\ndf.columns\n# df = df[['Unnamed: 0','book','topic','content']]","metadata":{"execution":{"iopub.status.busy":"2024-06-20T06:33:38.945542Z","iopub.execute_input":"2024-06-20T06:33:38.945835Z","iopub.status.idle":"2024-06-20T06:33:40.279288Z","shell.execute_reply.started":"2024-06-20T06:33:38.945805Z","shell.execute_reply":"2024-06-20T06:33:40.278397Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index(['book', 'topic', 'content'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"from qdrant_client import models, QdrantClient\nfrom sentence_transformers import SentenceTransformer\nfrom qdrant_client.http import models\n\nclient = QdrantClient(url=\"https://ccaa5582-c81c-49c3-93a9-866c4617ce0e.us-east4-0.gcp.cloud.qdrant.io:6333\",\n    api_key=\"S1RryvNzKIXBwLYriv2FVqj11oUgoyrrxyLJDNfiyi5he7b6SRs-jg\",)\nmodelst = SentenceTransformer(\"msmarco-distilbert-dot-v5\")\n\n# client.create_collection(\n#     collection_name=\"database\",\n#     hnsw_config=models.HnswConfigDiff(\n#         m=81,\n#         ef_construct=200,\n#     ),\n#     vectors_config=models.VectorParams(\n#         size=768,\n#         distance=models.Distance.COSINE,\n#     ),\n# )","metadata":{"execution":{"iopub.status.busy":"2024-06-20T06:33:40.280785Z","iopub.execute_input":"2024-06-20T06:33:40.281147Z","iopub.status.idle":"2024-06-20T06:34:04.575849Z","shell.execute_reply.started":"2024-06-20T06:33:40.281116Z","shell.execute_reply":"2024-06-20T06:34:04.575035Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"client.upload_points(\n    collection_name=\"database\",\n    points=[\n        models.PointStruct(\n            id=i,\n            vector=modelst.encode(str(obj['content'])).tolist(),\n            payload=dict(obj)\n        )\n        for i,obj in temp.iterrows()\n    ],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client.count(\n    collection_name='database'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def user_search(qn):\n    \n    hits = client.search(\n        collection_name=\"database\",\n        query_vector=modelst.encode(qn).tolist(),\n        limit=10,\n        search_params=models.SearchParams(hnsw_ef=128,exact=True),\n    )\n\n    res = []\n    \n    print('\\nTop 5 results!!\\n')\n    \n    for hit in hits:\n        \n        if hit.score > 77:\n            res.append(hit.payload)\n            \n        print(hit.payload)\n        print('score = ',hit.score,'\\n')\n        \n    return res","metadata":{"execution":{"iopub.status.busy":"2024-06-20T06:34:27.240682Z","iopub.execute_input":"2024-06-20T06:34:27.241296Z","iopub.status.idle":"2024-06-20T06:34:27.247227Z","shell.execute_reply.started":"2024-06-20T06:34:27.241265Z","shell.execute_reply":"2024-06-20T06:34:27.246322Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# from sentence_transformers.cross_encoder import CrossEncoder\n# import numpy as np\n\n# ranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n\n# def ranking(query,paras):\n#     texts = [(query,para['content']) for para in paras]\n#     scores = ranker.predict(texts)\n\n#     # Sort the scores in decreasing order to get the corpus indices\n#     ranked_indices = np.argsort(scores)[::-1]\n\n#     print('after re-ordering\\n')\n#     for r in ranked_indices:\n#         print(paras[r],'\\n score = ',scores[r],'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U accelerate\n!pip install -q -U bitsandbytes\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nimport torch\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True, #changing the model weights to bit floating points\n    bnb_4bit_compute_dtype=torch.float16, #the computational matrices that will be used for training or inference in floating point 16\n    bnb_4bit_quant_type=\"nf4\", #this is the quantization type\n    bnb_4bit_use_double_quant=True, #doing double quantization\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T06:34:06.065334Z","iopub.status.idle":"2024-06-20T06:34:06.065820Z","shell.execute_reply.started":"2024-06-20T06:34:06.065563Z","shell.execute_reply":"2024-06-20T06:34:06.065582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\",quantization_config=config,device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(context,query):\n    \n#     print('entered for generation')\n    global messages \n        \n    prompt = f\"\"\"\n    Chat History is given below.\n    ---------------------\n    {messages}\n    ---------------------\n    Context information is below.\n    ---------------------\n    Context:\n    {context}\n    ---------------------\n    Given the context and chat history, answer the query.\n    Query: {query}\n    Instructions:\n        - Generate a comprehensive response by leveraging the context effectively to address the query cohesively, ensuring the information provided is relevant and coherent within the given context. \n        - Leverage the usage of transliterations,romanizations, anecdotes or short stories from the context to answer the query.    \n        - When the context is \"None,\" utilize the chat history as a reference to guide the generation of new text, focusing on creating original content while avoiding direct duplication of previous dialogues.\n        - Do not stop the text generation abruptly in the middle of a sentence. Ensure that the reponse ends with good meaning and coherence.\n        - Once in a while, greet the user with 'Sairam!' or 'Embodiment of Divine Love!!' to establish a friendly tone.\n        - Embody the sacred duty of a dedicated messenger, entrusted with the profound responsibility of imparting the timeless teachings and profound wisdom of Bhagawan Sri Sathya Sai Baba with unwavering devotion and reverence.\n        - Do not use your own knowledge for answering. If the context provided doesn't suffice or isn't aligned with the query, kindly conclude by using, \"In my continuous learning journey, I strive to provide accurate responses. However, if the context is unclear, I may not be able to provide a relevant answer. Please consider refining your query for better assistance.\"\n\n    Answer:\n    \"\"\"\n    \n    encodeds = tokenizer(prompt, return_tensors=\"pt\")\n    model_inputs = encodeds.to(\"cuda\")\n\n    generated_ids = model.generate(\n        **model_inputs,\n        num_beams=2,\n        max_new_tokens=4096,\n        do_sample=True,\n        early_stopping=True,\n        repetition_penalty = 3.99,\n    )\n\n    decoded_text = tokenizer.batch_decode(generated_ids)\n    i = decoded_text[0].find('Answer:') + 7\n    ans = decoded_text[0][i:]\n    ans.replace('</s>',' ')\n    ans.replace('\\n','.')\n    \n    messages.append('User:'+query)\n    messages.append('Assistant:'+decoded_text[0][i:])\n    \n    while len(messages) > 5:\n        messages.pop(0)\n        \n    print('\\nlength of generated text is ',len(decoded_text[0][i:]),'\\n\\n')\n    \n    return decoded_text[0][i:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_opinion(query):\n    \n    phrases = [\"you\",\"yours\",\"yourself\",\" u \",\" urs \",\" urself \",\"your\",\" ur \",\"earlier\",\"before\",\"going back\",\"you repeated\",\"previous\",\"previously\",\"we discussed\",\"it\"]\n    \n    for phrase in phrases:\n        \n        if phrase in query:\n            return True\n        \n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    query = input('Enter your query',)\n    context = 'None'\n    \n    if not check_opinion(query):\n        context = user_search(query)\n    \n    generated_text = generate(context,query)\n        \n    print('-'*100)\n    print(\"\\nThe model's generated text for the above context is : \\n\",generated_text.replace('</s>',\" \"))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global messages\nmessages","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}